{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JcgzUoZNH8h"
      },
      "source": [
        "# Использование предобученных трансформеров"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers[torch]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRWgMitFfbVi",
        "outputId": "ce013e0c-a2df-4d05-81dc-80f00ef76c21"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers[torch]\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers[torch])\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers[torch])\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers[torch])\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.65.0)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.0.1+cu118)\n",
            "Collecting accelerate>=0.20.2 (from transformers[torch])\n",
            "  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.2->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (16.0.6)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.9->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.9->transformers[torch]) (1.3.0)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers, accelerate\n",
            "Successfully installed accelerate-0.20.3 huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers[sentencepiece]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtKQPd-V-hLn",
        "outputId": "206d94a3-8f18-4d3b-815c-d731da8f76e9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (4.65.0)\n",
            "Collecting sentencepiece!=0.1.92,>=0.1.91 (from transformers[sentencepiece])\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf<=3.20.3 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[sentencepiece]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[sentencepiece]) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.4)\n",
            "Installing collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "scrolled": true,
        "id": "FVWGOjeyNH8u"
      },
      "outputs": [],
      "source": [
        "# стандартные библиотеки\n",
        "import os, re\n",
        "import numpy as np\n",
        "from time import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from string import punctuation\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "# tf и huggingface\n",
        "import tensorflow as tf\n",
        "from transformers import TFAutoModel\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "DEVICE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZdG1XlYUQA3",
        "outputId": "f7310402-c4aa-407f-910c-4ea7f392e83e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQVgCvssNH8u"
      },
      "source": [
        "Возьмем данные lenta.ru, но не целиком. Fine-tuning больших моделей лучше всего подходит, когда данных совсем мало и стандартным алгоритмам просто не хватает информации, чтобы обучиться. Поэтому возьмем только небольшой процент всех данных."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vMSvaAHOhVJ",
        "outputId": "a3b94304-0191-441b-c97d-3abea0502e0c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "q479Nv1sNH8v"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/ВШЭ/Магистратура/NLP/lenta_sample.csv')\n",
        "data.dropna(subset=['topic', 'text'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HonzFeR4NH8z",
        "outputId": "a5d908a2-32e0-4d97-d908-833832b7663c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(607, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfsEwmgcNH82"
      },
      "source": [
        "Будем обучаться на заголовках, а не на самих текстах"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.title.values[:5], data.topic[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z15kEQsGTgKb",
        "outputId": "7f13bc88-aaad-4111-ae1e-a02b3d96a9ec"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array(['Московская милиция ужесточила паспортный режим',\n",
              "        'Московского студента ограбили на\\xa06\\xa0миллионов рублей',\n",
              "        'В Ставропольском крае обезврежены боевики',\n",
              "        'Лужков отказался трудоустраивать китайцев с\\xa0Черкизовского рынка',\n",
              "        'По факту пожара на\\xa0заводе в\\xa0Югре заведено дело'],\n",
              "       dtype=object),\n",
              " 0    Россия\n",
              " 1    Россия\n",
              " 2    Россия\n",
              " 3    Россия\n",
              " 4    Россия\n",
              " Name: topic, dtype: object)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = data.title.values\n",
        "id2label = {i:l for i,l in enumerate(set(data.topic))}\n",
        "label2id = {l:i for i,l in id2label.items()}\n",
        "targets = [label2id[l] for l in data.topic]"
      ],
      "metadata": {
        "id": "o7LnnrG5TaCw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts, test_texts, train_targets, test_targets = train_test_split(texts, targets, test_size=0.05, stratify=targets)"
      ],
      "metadata": {
        "id": "UFjjzURWW3BU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_texts[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQPoVi03YE_r",
        "outputId": "4c703ac9-5819-414c-f1f4-5dd72b1370e9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['\"Кассини\" сфотографировал восход спутников над кольцами Сатурна',\n",
              "       'Maybelline снимет в\\xa0рекламе модель-азиатку И-Хуа У',\n",
              "       'Еврокомиссия предложила снизить НДС для ресторанов и\\xa0парикмахерских',\n",
              "       'Полное уничтожение элитной гостиницы за\\xa010\\xa0секунд попало на\\xa0видео',\n",
              "       'На юге Англии появилось фальшивое граффити Бэнкси'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_targets[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HPBP0s0YR6w",
        "outputId": "c6129df8-bd7d-4c5b-df51-b67f043a263c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4, 15, 8, 6, 7]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts = train_texts.tolist()\n",
        "test_texts = test_texts.tolist()"
      ],
      "metadata": {
        "id": "RTIiQQhgZYv4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 512"
      ],
      "metadata": {
        "id": "oo-5zVQuWesd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ],
      "metadata": {
        "id": "-VxHg9gKbnDF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxpelWYpNH8z"
      },
      "source": [
        "Список всех доступных моделей можно найти тут - https://huggingface.co/models  \n",
        "А вот тут основные с описанием - https://huggingface.co/transformers/pretrained_models.html"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DeepPavlov/rubert-base-cased"
      ],
      "metadata": {
        "id": "sue4skl2WhNy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D61A87tyNH80",
        "outputId": "11b8e62e-6be9-405d-a51f-4f468923f928"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "tokenizer_dp = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
        "model_dp = AutoModelForSequenceClassification.from_pretrained(\"DeepPavlov/rubert-base-cased\", num_labels=len(label2id)).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_enc_dp = tokenizer_dp(train_texts, truncation=True, padding=True, max_length=MAX_LEN)\n",
        "test_enc_dp = tokenizer_dp(test_texts, truncation=True, padding=True, max_length=MAX_LEN)"
      ],
      "metadata": {
        "id": "8GP_Ivn7V_o2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Dataset(train_enc_dp, train_targets)\n",
        "test_dataset = Dataset(test_enc_dp, test_targets)"
      ],
      "metadata": {
        "id": "SCUnoKI8bTT-"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.thepythoncode.com/code/finetuning-bert-using-huggingface-transformers-python\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=15,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=20,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=200,\n",
        "    evaluation_strategy=\"steps\",\n",
        ")"
      ],
      "metadata": {
        "id": "L-7CAz1tbZLm"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model_dp,                         # the instantiated Transformers model to be trained\n",
        "    args=training_args,                     # training arguments, defined above\n",
        "    train_dataset=train_dataset,            # training dataset\n",
        "    eval_dataset=test_dataset,              # evaluation dataset\n",
        "    )"
      ],
      "metadata": {
        "id": "f3BiGT3_bKof"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "5DaxsgO4b8dA",
        "outputId": "da7403b8-b0e6-4613-bb89-0210dc5f884c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1080' max='1080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1080/1080 02:44, Epoch 15/15]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.184000</td>\n",
              "      <td>1.519929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.044000</td>\n",
              "      <td>2.553760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.031300</td>\n",
              "      <td>2.487431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.007400</td>\n",
              "      <td>2.858083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.011900</td>\n",
              "      <td>2.615438</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1080, training_loss=0.051691612463306495, metrics={'train_runtime': 164.776, 'train_samples_per_second': 52.435, 'train_steps_per_second': 6.554, 'total_flos': 97693134739200.0, 'train_loss': 0.051691612463306495, 'epoch': 15.0})"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = trainer.predict(test_dataset)\n",
        "pred = list(np.argmax(predictions.predictions, axis=-1))\n",
        "print(classification_report(test_targets, pred, labels=list(range(len(label2id))),\n",
        "                            target_names=list(label2id), zero_division=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "ygK0KKVkij9Q",
        "outputId": "98a7b27a-2004-4b77-a2ea-2806720b9b2b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   precision    recall  f1-score   support\n",
            "\n",
            "        Экономика       0.50      0.50      0.50         2\n",
            "           Россия       0.00      0.00      0.00         2\n",
            "             Крым       0.00      0.00      0.00         0\n",
            "          Легпром       0.50      1.00      0.67         1\n",
            "Силовые структуры       0.67      1.00      0.80         2\n",
            "              Мир       0.00      0.00      0.00         1\n",
            "  Наука и техника       0.60      1.00      0.75         3\n",
            "         Культура       0.50      0.33      0.40         3\n",
            "         Из жизни       1.00      0.33      0.50         3\n",
            "         Ценности       0.50      0.50      0.50         2\n",
            "            Спорт       1.00      1.00      1.00         2\n",
            "           Бизнес       0.00      0.00      0.00         2\n",
            "   Интернет и СМИ       1.00      1.00      1.00         2\n",
            "   69-я параллель       0.50      1.00      0.67         1\n",
            "       Библиотека       0.00      0.00      0.00         0\n",
            "              Дом       1.00      0.50      0.67         2\n",
            "      Бывший СССР       0.50      0.33      0.40         3\n",
            "\n",
            "        micro avg       0.55      0.55      0.55        31\n",
            "        macro avg       0.49      0.50      0.46        31\n",
            "     weighted avg       0.58      0.55      0.53        31\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "А как нарисовать красивый график с историей обучения?"
      ],
      "metadata": {
        "id": "AazjAjLekjZn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRR-EQxjNH84"
      },
      "source": [
        "xlm-roberta-base давала f1 = 0.1 почему-то\n",
        "\n",
        "cointegrated/rubert-tiny давала f1 = 0.26\n",
        "\n",
        "mt5 и LaBSE не запустились\n",
        "\n",
        "### cointegrated/rubert-tiny2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "HhKnQgZ5NH88"
      },
      "outputs": [],
      "source": [
        "tokenizer_coin = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
        "model_coin = AutoModelForSequenceClassification.from_pretrained(\"cointegrated/rubert-tiny2\", num_labels=len(label2id)).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 512"
      ],
      "metadata": {
        "id": "Su15qqUoq2tN"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ccfDgK9VNH88"
      },
      "outputs": [],
      "source": [
        "train_enc_coin = tokenizer_coin(train_texts, truncation=True, padding=True, max_length=MAX_LEN)\n",
        "test_enc_coin = tokenizer_coin(test_texts, truncation=True, padding=True, max_length=MAX_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Dataset(train_enc_coin, train_targets)\n",
        "test_dataset = Dataset(test_enc_coin, test_targets)"
      ],
      "metadata": {
        "id": "j2NO4OQr5ygI"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.thepythoncode.com/code/finetuning-bert-using-huggingface-transformers-python\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=30,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=20,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=200,\n",
        "    evaluation_strategy=\"steps\",\n",
        ")"
      ],
      "metadata": {
        "id": "ck0omcKDl82d"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_coin = Trainer(\n",
        "    model=model_coin,                         # the instantiated Transformers model to be trained\n",
        "    args=training_args,                     # training arguments, defined above\n",
        "    train_dataset=train_dataset,            # training dataset\n",
        "    eval_dataset=test_dataset,              # evaluation dataset\n",
        "    )"
      ],
      "metadata": {
        "id": "g6JuFUbil9LL"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_coin.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "AhNG7v4DmM88",
        "outputId": "13de7e6f-72ea-49e7-b071-a4dd785c6d80"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2160' max='2160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2160/2160 00:57, Epoch 30/30]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.803800</td>\n",
              "      <td>2.727407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>2.524200</td>\n",
              "      <td>2.277668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.810900</td>\n",
              "      <td>1.761022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.078400</td>\n",
              "      <td>1.499253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.573800</td>\n",
              "      <td>1.439064</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.289500</td>\n",
              "      <td>1.440316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.155600</td>\n",
              "      <td>1.413124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.096000</td>\n",
              "      <td>1.442928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.065700</td>\n",
              "      <td>1.472498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.053400</td>\n",
              "      <td>1.545860</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2160, training_loss=0.8786867832695996, metrics={'train_runtime': 57.5911, 'train_samples_per_second': 300.046, 'train_steps_per_second': 37.506, 'total_flos': 6234156576000.0, 'train_loss': 0.8786867832695996, 'epoch': 30.0})"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = trainer_coin.predict(test_dataset)\n",
        "pred = list(np.argmax(predictions.predictions, axis=-1))\n",
        "print(classification_report(test_targets, pred, labels=list(range(len(label2id))),\n",
        "                            target_names=list(label2id), zero_division=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "OLXJweRal9UG",
        "outputId": "2fb34705-775b-4e6c-db17-1d7549212bb9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   precision    recall  f1-score   support\n",
            "\n",
            "              Мир       0.00      0.00      0.00         1\n",
            "   Интернет и СМИ       0.40      1.00      0.57         2\n",
            "      Бывший СССР       0.00      0.00      0.00         3\n",
            "            Спорт       0.67      1.00      0.80         2\n",
            "  Наука и техника       1.00      0.67      0.80         3\n",
            "   69-я параллель       0.00      0.00      0.00         1\n",
            "         Из жизни       1.00      0.67      0.80         3\n",
            "         Культура       1.00      1.00      1.00         3\n",
            "        Экономика       0.25      0.50      0.33         2\n",
            "             Крым       0.00      0.00      0.00         0\n",
            "           Бизнес       0.50      0.50      0.50         2\n",
            "          Легпром       0.00      0.00      0.00         1\n",
            "       Библиотека       0.00      0.00      0.00         0\n",
            "              Дом       0.67      1.00      0.80         2\n",
            "           Россия       0.50      0.50      0.50         2\n",
            "         Ценности       0.67      1.00      0.80         2\n",
            "Силовые структуры       1.00      1.00      1.00         2\n",
            "\n",
            "        micro avg       0.65      0.65      0.65        31\n",
            "        macro avg       0.45      0.52      0.46        31\n",
            "     weighted avg       0.59      0.65      0.59        31\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### sismetanin/ruroberta-ru-rusentitweet"
      ],
      "metadata": {
        "id": "4ek0_-6K8h4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"sismetanin/ruroberta-ru-rusentitweet\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"sismetanin/ruroberta-ru-rusentitweet\", num_labels=len(label2id), ignore_mismatched_sizes=True).to(DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IfvErP58U5K",
        "outputId": "f33c9749-3dcf-4063-bea5-6cda48eb8544"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at sismetanin/ruroberta-ru-rusentitweet and are newly initialized because the shapes did not match:\n",
            "- classifier.out_proj.weight: found shape torch.Size([5, 1024]) in the checkpoint and torch.Size([17, 1024]) in the model instantiated\n",
            "- classifier.out_proj.bias: found shape torch.Size([5]) in the checkpoint and torch.Size([17]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_enc = tokenizer(train_texts, truncation=True, padding=True, max_length=MAX_LEN)\n",
        "test_enc = tokenizer(test_texts, truncation=True, padding=True, max_length=MAX_LEN)"
      ],
      "metadata": {
        "id": "tT8Cwx2C8mLH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Dataset(train_enc, train_targets)\n",
        "test_dataset = Dataset(test_enc, test_targets)"
      ],
      "metadata": {
        "id": "mRtS04zT8qMO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.thepythoncode.com/code/finetuning-bert-using-huggingface-transformers-python\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=30,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=20,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=200,\n",
        "    evaluation_strategy=\"steps\",\n",
        ")"
      ],
      "metadata": {
        "id": "TpCGllVG8xVs"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated Transformers model to be trained\n",
        "    args=training_args,                     # training arguments, defined above\n",
        "    train_dataset=train_dataset,            # training dataset\n",
        "    eval_dataset=test_dataset,              # evaluation dataset\n",
        "    )"
      ],
      "metadata": {
        "id": "c3Ri7ysC82XR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "emA9Zq_c88Jy",
        "outputId": "5fb440db-772b-4673-f41f-5d06ea62b8fe"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2160' max='2160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2160/2160 12:23, Epoch 30/30]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.372800</td>\n",
              "      <td>1.574950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.777200</td>\n",
              "      <td>1.538012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.286200</td>\n",
              "      <td>2.737845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.117900</td>\n",
              "      <td>3.442244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.059100</td>\n",
              "      <td>2.300742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.018500</td>\n",
              "      <td>2.273026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>2.513854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.000400</td>\n",
              "      <td>2.458716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>2.451779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.003600</td>\n",
              "      <td>2.267057</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2160, training_loss=0.3367229464757084, metrics={'train_runtime': 747.1541, 'train_samples_per_second': 23.128, 'train_steps_per_second': 2.891, 'total_flos': 1006536882954240.0, 'train_loss': 0.3367229464757084, 'epoch': 30.0})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = trainer.predict(test_dataset)\n",
        "pred = list(np.argmax(predictions.predictions, axis=-1))\n",
        "print(classification_report(test_targets, pred, labels=list(range(len(label2id))),\n",
        "                            target_names=list(label2id), zero_division=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "y-00I2078_3j",
        "outputId": "65db994c-44be-4b7a-bcb5-9eacf657f6cc"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   precision    recall  f1-score   support\n",
            "\n",
            "           Россия       0.50      0.50      0.50         2\n",
            "          Легпром       1.00      1.00      1.00         1\n",
            "           Бизнес       0.67      1.00      0.80         2\n",
            "      Бывший СССР       1.00      0.67      0.80         3\n",
            "         Из жизни       0.67      0.67      0.67         3\n",
            "       Библиотека       0.00      0.00      0.00         0\n",
            "   69-я параллель       1.00      1.00      1.00         1\n",
            "        Экономика       1.00      1.00      1.00         2\n",
            "   Интернет и СМИ       1.00      0.50      0.67         2\n",
            "            Спорт       1.00      1.00      1.00         2\n",
            "Силовые структуры       1.00      1.00      1.00         2\n",
            "         Ценности       0.00      0.00      0.00         2\n",
            "         Культура       0.50      0.67      0.57         3\n",
            "  Наука и техника       1.00      0.67      0.80         3\n",
            "              Дом       1.00      0.50      0.67         2\n",
            "             Крым       0.00      0.00      0.00         0\n",
            "              Мир       0.33      1.00      0.50         1\n",
            "\n",
            "        micro avg       0.71      0.71      0.71        31\n",
            "        macro avg       0.69      0.66      0.65        31\n",
            "     weighted avg       0.78      0.71      0.72        31\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "У этой модели лучше всего получилось."
      ],
      "metadata": {
        "id": "wpwRnQP2uvB3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Чем ROBERTA отличается от BERT? В какой статье описана ROBERTA?\n",
        "\n",
        "2. Чем модель T5 отличается от ROBERTA/BERT?\n",
        "\n",
        "RoBERTa - [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)\n",
        "\n",
        "RoBERTa - хорошо натренированный берт, с большим числом гиперпараметров. У неё лучше качество, чем у берта. Она тренировалась дольше, на более длинных последовательностях, батчи были длиннее, данных было больше. Маскирование у роберты динамическое, то есть на разных этапах обучения разные токены маскируются.\n",
        "\n",
        "\n",
        "T5 - энкодер-декодер модель, (Ro)BERT(a) - энкодер модель. (Ro)BERT(a) учились угадывать маскированное слово и чувствовать связь между предложениями. T5 учили предсказывать последовательность слов, поэтому она хорошо справляется с задачами, на текст отвечает подходящим текстом. (особенно здорово, если модель учили на нескольких задачах сразу)"
      ],
      "metadata": {
        "id": "kAnVZBDOZ4Iq"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}