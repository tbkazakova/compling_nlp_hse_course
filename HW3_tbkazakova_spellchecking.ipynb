{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "371970ff",
   "metadata": {},
   "source": [
    "# Домашнее задание № 3. Исправление опечаток"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35cf8bd",
   "metadata": {},
   "source": [
    "## 1. Доп. ранжирование по вероятности (3 балла)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f94d0b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "886cb0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install textdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d21dfe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install razdel tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef15aaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "from nltk import sent_tokenize\n",
    "punctuation += \"«»—…“”\"\n",
    "punct = set(punctuation)\n",
    "import textdistance\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
    "from string import punctuation\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16cbb76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1eb5d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad = open('data/sents_with_mistakes.txt', encoding='utf8').read().splitlines()\n",
    "true = open('data/correct_sents.txt', encoding='utf8').read().splitlines()\n",
    "corpus = open('data/wiki_data.txt', encoding='utf8').read()\n",
    "vocab = Counter(re.findall('\\w+', corpus.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61159c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# напишем функцию, которая будет сопоставлять слова в правильном и ошибочном варианте\n",
    "# разобьем предложение по пробелам и удалим пунктуация на границах слов\n",
    "def align_words(sent_1, sent_2):\n",
    "    tokens_1 = sent_1.lower().split()\n",
    "    tokens_2 = sent_2.lower().split()\n",
    "    \n",
    "    tokens_1 = [token.strip(punctuation) for token in tokens_1]\n",
    "    tokens_2 = [token.strip(punctuation) for token in tokens_2]\n",
    "    \n",
    "    tokens_1 = [token for token in tokens_1 if token]\n",
    "    tokens_2 = [token for token in tokens_2 if token]\n",
    "    \n",
    "    assert len(tokens_1) == len(tokens_2)\n",
    "    \n",
    "    return list(zip(tokens_1, tokens_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbc76c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = list(vocab.keys())\n",
    "id2word = {i:word for i, word in enumerate(vocab)}\n",
    "\n",
    "\n",
    "vec = CountVectorizer(analyzer='char', ngram_range=(1,1))\n",
    "X = vec.fit_transform(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5e30b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_match_vec(text, X, vec, topn=20):\n",
    "    v = vec.transform([text])\n",
    "    similarities = cosine_distances(v, X)[0]\n",
    "    topn = similarities.argsort()[:topn] \n",
    "    \n",
    "    return [(id2word[top], similarities[top]) for top in topn]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6be25c",
   "metadata": {},
   "source": [
    "Дополните get_closest_hybrid_match в семинаре так, чтобы из кандадатов с одинаковым расстоянием редактирования выбиралось наиболее вероятное."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8e8814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = sum(vocab.values())\n",
    "\n",
    "def P(word, N=N):\n",
    "    return vocab[word] / N\n",
    "\n",
    "def get_closest_match_with_metric(text, lookup, topn=20, metric=textdistance.levenshtein):\n",
    "    similarities = Counter()\n",
    "    for word in lookup:\n",
    "        similarities[word] = metric.normalized_similarity(text, word)\n",
    "    vars2sort = similarities.most_common(topn)\n",
    "    \n",
    "    words, sims = zip(*vars2sort)\n",
    "    visited = set()\n",
    "    dupl = set(x for x in sims if x in visited or (visited.add(x) or False))\n",
    "    # список повторяющихся расстояний редактирования\n",
    "\n",
    "    similarities_freqcorrected = Counter()  \n",
    "    got_similarity = None\n",
    "    index = 1\n",
    "    word_freq = Counter() # для слов с повторяющимся расстоянием редактирования: слово-частотность\n",
    "    for word, similarity in vars2sort:\n",
    "        if similarity in dupl:\n",
    "            word_freq[word] = P(word)\n",
    "            # логарифм вероятности не использую, потому что и так занулиться ничего не должно\n",
    "        else:\n",
    "            for word_to_sort in word_freq.most_common():\n",
    "                similarities_freqcorrected[word_to_sort[0]] = index\n",
    "                index+=1\n",
    "            word_freq = Counter()\n",
    "            similarities_freqcorrected[word] = index\n",
    "            index+=1\n",
    "    if word_freq != Counter():\n",
    "        for word_to_sort in word_freq.most_common():\n",
    "                similarities_freqcorrected[word_to_sort[0]] = index\n",
    "                index+=1\n",
    "\n",
    "    return similarities_freqcorrected.most_common()[:-topn-1:-1]\n",
    "\n",
    "def get_closest_hybrid_match(text, X, vec, topn=3, metric=textdistance.damerau_levenshtein):\n",
    "    candidates = get_closest_match_vec(text, X, vec, topn*4)\n",
    "    lookup = [cand[0] for cand in candidates]\n",
    "    closest = get_closest_match_with_metric(text, lookup, topn, metric=metric)\n",
    "\n",
    "    return closest\n",
    "\n",
    "def predict_mistaken(word, vocab):\n",
    "    return 0 if word in vocab else 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93059911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('солнце', 1), ('соне', 2), ('сосновец', 3)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_closest_hybrid_match('сонце', X, vec)\n",
    "# Мы тут не сохранили вывод расстояния редактирования, но нас и не просили"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4a03e8",
   "metadata": {},
   "source": [
    "Оценим качество"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1127c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20245da2af144d17b85d4341dea08b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/915 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mistakes = []\n",
    "total_mistaken = 0\n",
    "mistaken_fixed = 0\n",
    "\n",
    "total_correct = 0\n",
    "correct_broken = 0\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "cashed = {}\n",
    "for i in tqdm(range(len(true))):\n",
    "    word_pairs = align_words(true[i], bad[i])\n",
    "    for pair in word_pairs:\n",
    "        if predict_mistaken(pair[1], vocab):\n",
    "            pred = cashed.get(pair[1], get_closest_hybrid_match(pair[1], X, vec)[0][0])\n",
    "            cashed[pair[1]] = pred\n",
    "        else:\n",
    "            pred = pair[1]\n",
    "        \n",
    "            \n",
    "        if pred == pair[0]:\n",
    "            correct += 1\n",
    "        else:\n",
    "            mistakes.append((pair[0], pair[1], pred))\n",
    "        total += 1\n",
    "            \n",
    "        if pair[0] == pair[1]:\n",
    "            total_correct += 1\n",
    "            if pair[0] != pred:\n",
    "                correct_broken += 1\n",
    "        else:\n",
    "            total_mistaken += 1\n",
    "            if pair[0] == pred:\n",
    "                mistaken_fixed += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41cd345f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8477238619309655\n",
      "0.42701863354037267\n",
      "0.09004249454461927\n"
     ]
    }
   ],
   "source": [
    "print(correct/total)\n",
    "print(mistaken_fixed/total_mistaken)\n",
    "print(correct_broken/total_correct)\n",
    "# Без добавления ранжирования по вероятности качество такое\n",
    "# 0.847023511755878\n",
    "# 0.421583850931677\n",
    "# 0.09004249454461927"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f96c148",
   "metadata": {},
   "source": [
    "Результат улучшился, но незначительно"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cf9985",
   "metadata": {},
   "source": [
    "## 2.  Symspell (5 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9392cc23",
   "metadata": {},
   "source": [
    "Реализуйте алгоритм Symspell. Он похож на алгоритм Норвига, но проще и быстрее. Там к словам применяется только одна операция - удаление символа. Описание алгоритма по шагам:\n",
    "\n",
    "1) Составляется словарь правильных слов  \n",
    "2) На основе словаря правильных слов составляется словарь удалений - для каждого правильного слова создаются все варианты удалений и создается словарь, где ключ - слово с удалением, а значение - правильное слово   \n",
    "3) Для выбора исправления для слова с опечаткой генерируются все варианты удаления, из них выбираются те, что есть в словаре удалений, построенного на шаге 2. Слово с опечаткой заменяется на правильное слово, соответствующее варианту удаления  \n",
    "4) Если в словаре удалений есть несколько вариантов, то выбирается удаление, которому соответствует наиболее вероятное правильное слово  \n",
    "\n",
    "\n",
    "Оцените качество полученного алгоритма теми же тремя метриками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96c9fae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open('data/wiki_data.txt', encoding='utf8').read()\n",
    "vocab = Counter(re.findall('\\w+', corpus.lower()))  # Составляется словарь правильных слов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857ef9cc",
   "metadata": {},
   "source": [
    "Удалять можно ровно 1 символ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f92c09c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrections = {}\n",
    "for word in vocab:\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    deletes = [L + R[1:] for L, R in splits if R]\n",
    "    for delete in deletes:\n",
    "        if delete in corrections:\n",
    "            corrections[delete].append(word)\n",
    "        else:\n",
    "            corrections[delete] = [word]\n",
    "# Составляется словарь удалений\n",
    "# по условию может быть несколько вариантов правильных слов для слова с опечаткой,\n",
    "# поэтому  у нас не \"ключ - слово с удалением, а значение - правильное слово\",\n",
    "# а \"ключ - слово с удалением, а значение - список правильных слов\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e9aebde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corrected(word):\n",
    "    if word in corrections:\n",
    "        if len(corrections[word]) > 1:\n",
    "            word_freq = Counter()\n",
    "            for var in corrections[word]:\n",
    "                word_freq[var] = P(var)\n",
    "            corrected = word_freq.most_common(1)[0][0]\n",
    "        else:\n",
    "            corrected = corrections[word][0]\n",
    "    else:\n",
    "        corrected = word\n",
    "    return corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b1ec3c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb9e6bd6d58439b8059eb429db8a0d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/915 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "получатся полчатся получатся\n",
      "отсутствие отсуствие отсутствие\n",
      "основная основая основная\n",
      "напрасно нарасно напрасно\n",
      "предлагаю предлагю предлагаю\n",
      "сегодняшнее сегодяшнее сегодняшнее\n",
      "лучше лчше лучше\n",
      "компьютерная компютерная компьютерная\n",
      "ответственность отвественность ответственность\n",
      "комментариев коментариев комментариев\n",
      "ответственный отвественный ответственный\n",
      "рассчитаны расчитаны рассчитаны\n",
      "расспрашивая распрашивая расспрашивая\n",
      "странно сранно странно\n",
      "вообще вобще вообще\n",
      "зубные зуные зубные\n",
      "сочувствующие сочуствующие сочувствующие\n",
      "рассказчик расказчик рассказчик\n",
      "приехали прихали приехали\n",
      "немного нмного немного\n",
      "пьяный пяный пьяный\n",
      "священнослужители священослужители священнослужители\n",
      "первый первй первый\n",
      "вообще вобще вообще\n",
      "гулливера гуливера гулливера\n",
      "одноклассниках однокласниках одноклассниках\n",
      "температура тепература температура\n",
      "симптомы симтомы симптомы\n",
      "потому потму потому\n",
      "одноклассников однокласников одноклассников\n",
      "профессор професор профессор\n",
      "сегодняшние сегодяшние сегодняшние\n",
      "рассмотрим расмотрим рассмотрим\n",
      "убрать убрть убрать\n",
      "ответственно ответсвенно ответственно\n",
      "мальчик малчик мальчик\n",
      "следующий следущий следующий\n",
      "вообще вобще вообще\n",
      "ошибочно ошибчно ошибочно\n",
      "электронная электонная электронная\n",
      "реклама реклма реклама\n",
      "небольшая небльшая небольшая\n",
      "подробный подрбный подробный\n",
      "вечером вечром вечером\n",
      "вообще вобще вообще\n",
      "обычным обчным обычным\n",
      "поднимать поднмать поднимать\n",
      "предположительно предпложительно предположительно\n",
      "объяснить обяснить объяснить\n",
      "привезла приезла привезла\n",
      "некоторым некотрым некоторым\n",
      "прямо прям прямо\n",
      "следующий следущий следующий\n",
      "постарайтесь постарайтеь постарайтесь\n",
      "сумасшедшего сумашедшего сумасшедшего\n",
      "чувствовалась чувстовалась чувствовалась\n",
      "поссорилась посорилась поссорилась\n",
      "сначала сначла сначала\n",
      "будешь будеш будешь\n",
      "вообще вобще вообще\n",
      "следующие следущие следующие\n",
      "некоторые некотрые некоторые\n",
      "смешные смшные смешные\n",
      "совершенно совршенно совершенно\n",
      "пассажирам пасажирам пассажирам\n",
      "замечательные замечаельные замечательные\n",
      "вообще вобще вообще\n",
      "сегодня сгодня сегодня\n",
      "стрельбище стельбище стрельбище\n",
      "сентябрьский сентябрский сентябрьский\n",
      "целью целю целью\n",
      "скульптура сульптура скульптура\n",
      "территория терртория территория\n",
      "программка програмка программка\n",
      "половозрелые половозрлые половозрелые\n",
      "вообще вобще вообще\n",
      "рассчитан расчитан рассчитан\n",
      "реальные ральные реальные\n",
      "семантическая семантческая семантическая\n",
      "часть чать часть\n",
      "вообще вобще вообще\n",
      "обмороженных обмороженых обмороженных\n",
      "сказала скзала сказала\n",
      "сочетание сочетаие сочетание\n",
      "вообще вобще вообще\n",
      "приятно прияно приятно\n",
      "старыми тарыми старыми\n",
      "понимают понмают понимают\n",
      "чтобы чтоы чтобы\n",
      "спасибо пасибо спасибо\n",
      "пришлось пришлсь пришлось\n",
      "остается остется остается\n",
      "следующего следущего следующего\n",
      "вообще вобще вообще\n",
      "погода погда погода\n",
      "искусстве икусстве искусстве\n",
      "комплексное комлексное комплексное\n",
      "просмотреть просмотрет просмотреть\n",
      "впереди впреди впереди\n",
      "отсрочку осрочку отсрочку\n",
      "следующий следущий следующий\n",
      "больше болше больше\n",
      "опять опяь опять\n",
      "транспортировка траспортировка транспортировка\n",
      "удивительно удивительо удивительно\n",
      "наркотиков накотиков наркотиков\n",
      "проверку прверку проверку\n",
      "рассчитан расчитан рассчитан\n",
      "программу програму программу\n",
      "говорит говрит говорит\n",
      "правильная правльная правильная\n",
      "пространство постранство пространство\n",
      "продукты продкты продукты\n",
      "российская россиская российская\n",
      "обратно обатно обратно\n",
      "эксперты экперты эксперты\n",
      "методологическое методолгическое методологическое\n",
      "надеюсь ндеюсь надеюсь\n",
      "потому птому потому\n",
      "некоторые некторые некоторые\n",
      "может можт может\n",
      "рассчитаны расчитаны рассчитаны\n",
      "недавно недаво недавно\n",
      "привязанность првязанность привязанность\n",
      "приятного прияного приятного\n",
      "продавцы проавцы продавцы\n",
      "познакомились познакомиись познакомились\n",
      "попадались попадлись попадались\n",
      "немного нмного немного\n",
      "поселения поселеия поселения\n",
      "какая какя какая\n",
      "соответственно соотвественно соответственно\n",
      "вообще вобще вообще\n",
      "некоторые некторые некоторые\n",
      "неожиданно неожидано неожиданно\n",
      "вообще вобще вообще\n",
      "произошли проиошли произошли\n",
      "фаланги фалаги фаланги\n",
      "сердце серце сердце\n",
      "однако однао однако\n",
      "любви лбви любви\n",
      "транспортная траспортная транспортная\n",
      "продолжение продолжние продолжение\n",
      "создавалась создвалась создавалась\n",
      "приятно притно приятно\n",
      "приметы прметы приметы\n",
      "зарегистрироваться зарегистрироаться зарегистрироваться\n",
      "продолжение прдолжение продолжение\n",
      "соответственно соответсвенно соответственно\n",
      "вообще вобще вообще\n",
      "очень очеь очень\n",
      "перкуссионист перкуссинист перкуссионист\n",
      "одноклассники однокласники одноклассники\n",
      "расстоянии растоянии расстоянии\n",
      "лучше лчше лучше\n",
      "противоречие противречие противоречие\n",
      "накатило нактило накатило\n",
      "наверху нверху наверху\n",
      "следующий следуюий следующий\n",
      "представители предствители представители\n",
      "чувствую чувсвую чувствую\n",
      "лучше луче лучше\n",
      "может можт может\n",
      "миллиардер милиардер миллиардер\n",
      "следующее следущее следующее\n",
      "несколько несколко несколько\n",
      "вообще вобще вообще\n",
      "почувствовала почувстовала почувствовала\n",
      "удовольствие удоволствие удовольствие\n",
      "проанализировали проаализировали проанализировали\n",
      "вообще вобще вообще\n",
      "следующий следущий следующий\n",
      "баррикады барикады баррикады\n",
      "пришлось пршлось пришлось\n",
      "приехали прихали приехали\n",
      "отечественные отечественне отечественные\n",
      "коктейль коктель коктейль\n",
      "химиотерапии химиотерпии химиотерапии\n",
      "некоторые некотрые некоторые\n",
      "вообще вооще вообще\n",
      "непонятных непонятнх непонятных\n",
      "последняя поседняя последняя\n",
      "соответственно соответсвенно соответственно\n",
      "представьте представте представьте\n",
      "ответственность отвественность ответственность\n",
      "подливает подливет подливает\n",
      "вообще вобще вообще\n",
      "почувствовав почувстовав почувствовав\n",
      "сегодняшний сегодняшнй сегодняшний\n",
      "вообще вобще вообще\n",
      "одиночеством одночеством одиночеством\n",
      "оказывается оказвается оказывается\n",
      "важнейшие важейшие важнейшие\n",
      "заботились забоились заботились\n",
      "очищенный очищеный очищенный\n",
      "прямо прям прямо\n",
      "погода погда погода\n",
      "поверхностное поверхносное поверхностное\n",
      "очень очен очень\n",
      "центром центрм центром\n",
      "будешь будеш будешь\n",
      "мудрейший мудреший мудрейший\n",
      "некоторые неоторые некоторые\n",
      "прочитала прочтала прочитала\n",
      "прекратите прератите прекратите\n",
      "вообще вобще вообще\n"
     ]
    }
   ],
   "source": [
    "mistakes = []\n",
    "total_mistaken = 0\n",
    "mistaken_fixed = 0\n",
    "\n",
    "total_correct = 0\n",
    "correct_broken = 0\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "cashed = {}\n",
    "for i in tqdm(range(len(true))):\n",
    "    word_pairs = align_words(true[i], bad[i])\n",
    "    for pair in word_pairs:\n",
    "        if predict_mistaken(pair[1], vocab):\n",
    "            pred = cashed.get(pair[1], get_corrected(pair[1]))\n",
    "            cashed[pair[1]] = pred\n",
    "        else:\n",
    "            pred = pair[1]\n",
    "        \n",
    "            \n",
    "        if pred == pair[0]:\n",
    "            correct += 1\n",
    "        else:\n",
    "            mistakes.append((pair[0], pair[1], pred))\n",
    "        total += 1\n",
    "            \n",
    "        if pair[0] == pair[1]:\n",
    "            total_correct += 1\n",
    "            if pair[0] != pred:\n",
    "                correct_broken += 1\n",
    "        else:\n",
    "            total_mistaken += 1\n",
    "            if pair[0] == pred:\n",
    "                print(pair[0], pair[1], pred)\n",
    "                mistaken_fixed += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3385278a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8818409204602301\n",
      "0.15993788819875776\n",
      "0.011370161938670036\n"
     ]
    }
   ],
   "source": [
    "print(correct/total)\n",
    "print(mistaken_fixed/total_mistaken)\n",
    "print(correct_broken/total_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3add1a38",
   "metadata": {},
   "source": [
    "Нужно словарь больше брать. И токенизировать лучше. И align_words поправить. У нас вот такие куски в mistakes встречаются:\n",
    "\n",
    "    ('насчет', 'нащщот', 'нащщот'),\n",
    "    ('в', 'вобщем', 'вобщем'),\n",
    "    ('общем', 'как', 'как'),\n",
    "    ('как', 'вы', 'вы'),\n",
    "    ('вы', 'знаете', 'знаете'),\n",
    "    ('знаете', 'из', 'из'),\n",
    "    ('из', 'моего', 'моего'),\n",
    "    ('моего', 'не', 'не'),\n",
    "    ('недавнего', 'давнего', 'давнего'),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b292d96d",
   "metadata": {},
   "source": [
    "## *3. Чтение. (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee4b28f",
   "metadata": {},
   "source": [
    "Прочитайте эту главу в книге Speech and Language Processing - https://web.stanford.edu/~jurafsky/slp3/2.pdf .\n",
    "Ответьте на следующий вопрос:\n",
    "\n",
    "1. Что такое Byte-Pair Encoding (опишите по-русски, как минимум 10 предложениями)?\n",
    "\n",
    "*Это задание не связано напрямую с исправлением опечаток, но это важная тема, к которой мы вернемся в конце курса"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9194c702",
   "metadata": {},
   "source": [
    "BPE - алгоритм составления словаря символов для дальнейшей кодировки токенов.\n",
    "\n",
    "Можно назначать кодировку отдельным словам. Но что делать, если встретилось новое слово, которого не было, например, в тренировочной выборке? Можно попробовать всем-всем словам в мире назначить кодировку. Но тогда у нас будет бесконечный словарь. Держать кодировку для неизвестных слов? Можно назначить кодировку каждой букве. Тогда мы сможем кодировать любые слова, но получится, что буквы независимы друг от друга, а мы знаем, что какие-то сочетания встречаются чаще, у некоторых сочетаний есть понятные значения (у корней слов или других морфем, например).\n",
    "\n",
    "Поэтому можно использовать BPE. Сначала у нас в словаре буквы, которые встретились в тексте (+ символ конца слова, чтобы у нас не получились символы длиннее слов). Два символа, которые чаще всего встречаются вместе, объединим в сочетание - единый символ. И дальше будем объединять самые часто встречающиеся пары в отдельные символы. Благодаря этому, остановившись в любой момент, мы получим словарь символов, которыми можно закодировать любой текст (на знакомом нам языке, со знакомым нам алфавитом). Частотные слова будут для нас едиными символами целиком. Неизвестные слова разобьются на известные части, морфемы и в крайнем случае буквы. Для кодировки самых частотных сочетаний мы будем использовать меньше информации, чем нужно для побуквенной кодировки этих сочетаний. Наша кодировка будет осмысленнее, потому что морфемы и целые слова содержат больше значения, чем буквы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5809deb",
   "metadata": {},
   "source": [
    "P.S. Вы разрешили позже сдать. Я почти весь декабрь и новогодние каникулы болела (справка есть). Простите..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
