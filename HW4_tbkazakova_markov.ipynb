{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuSJoV0CQYdu"
      },
      "source": [
        "## Языковое моделирование\n",
        "\n",
        "В семинаре для генерации мы использовали предположение маркова и считали, что слово зависит только от 1 предыдущего слова. Но ничто нам не мешает попробовать увеличить размер окна и учитывать два или даже три прошлых слова. Для них мы еще сможем собрать достаточно статистик и, логично предположить, что качество сгенерированного текста должно вырасти.\n",
        "\n",
        "Попробуйте сделать языковую модель, которая будет учитывать два предыдущих слова при генерации текста. Сгенерируйте несколько текстов (3-5) и расчитайте перплексию получившейся модели. Можно использовать данные из семинара или любые другие (сопоставимые или большие по объему). Перплексию рассчитывайте на 10-50 отложенных предложениях (они не должны использоваться при сборе статистик).\n",
        "\n",
        "Подсказки:\n",
        "- нужно будет добавить еще один тэг\n",
        "- еще одна матрица не нужна, можно по строкам хранить биграмы, а по колонкам униграммы\n",
        "- тексты должны быть очень похожи на нормальные (если у вас получается рандомная каша, вы что-то делаете не так)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WWgQPX0wQYd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83fa9409-d505-4909-af20-c33ad0e9bcb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting razdel\n",
            "  Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: razdel\n",
            "Successfully installed razdel-0.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install razdel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "U3oLb80kQYd6"
      },
      "outputs": [],
      "source": [
        "from string import punctuation\n",
        "from razdel import sentenize\n",
        "from razdel import tokenize as razdel_tokenize\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from scipy.sparse import lil_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qgU22KP_9v1",
        "outputId": "94e68533-9249-4af3-82e8-c2da4c7c5bfa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DI3knbc1_fUP",
        "outputId": "14ee4f3e-0430-4ae9-b59d-6d75136b02c5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hc9im_OnQYd7"
      },
      "outputs": [],
      "source": [
        "news = open('/content/drive/MyDrive/ВШЭ/Магистратура/NLP/lenta.txt').read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkGCItJmQYd-"
      },
      "source": [
        "Напишем простую функцию для нормализации. Удалять пунктуацию и приводить к нижнему регистру, строго говоря не стоит, сгенерированный текст так будет не похож на настоящий. Но это немного упростит нам работу."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Vgdkvb-6QYd_"
      },
      "outputs": [],
      "source": [
        "def normalize(text):\n",
        "    normalized_text = [word.text.strip(punctuation) for word in razdel_tokenize(text)]\n",
        "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
        "    return normalized_text\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_news = [['<start>'] + ['<start>'] + normalize(text) + ['<end>'] + ['<end>'] for text in sent_tokenize(news[:5000000])]"
      ],
      "metadata": {
        "id": "eD2Ee4ztilKN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ixCnlHcFQYeT"
      },
      "outputs": [],
      "source": [
        "def ngrammer(tokens, n):\n",
        "    ngrams = []\n",
        "    for i in range(0,len(tokens)-n+1):\n",
        "        ngrams.append(' '.join(tokens[i:i+n]))\n",
        "    return ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bF7PiwVQQYeU"
      },
      "outputs": [],
      "source": [
        "unigrams_news = Counter()\n",
        "bigrams_news = Counter()\n",
        "trigrams_news = Counter()\n",
        "\n",
        "for sentence in sentences_news:\n",
        "    unigrams_news.update(sentence)\n",
        "    bigrams_news.update(ngrammer(sentence, 2))\n",
        "    trigrams_news.update(ngrammer(sentence, 3))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trigrams_news.most_common(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJHEFoFdAKAj",
        "outputId": "4267cab7-0d06-413a-da30-4e3f9c9d0ba2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('<start> <start> в', 3272),\n",
              " ('<start> <start> по', 2577),\n",
              " ('<start> <start> как', 1794),\n",
              " ('<start> <start> однако', 714),\n",
              " ('<start> <start> на', 684),\n",
              " ('<start> <start> об', 684),\n",
              " ('<start> об этом', 662),\n",
              " ('<start> <start> он', 653),\n",
              " ('<start> по словам', 630),\n",
              " ('<start> как сообщает', 611)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptehZM8jQYeX"
      },
      "source": [
        "### Генерация текста"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "pCIStHh8QYeZ"
      },
      "outputs": [],
      "source": [
        "matrix_news = lil_matrix((len(bigrams_news),\n",
        "                        len(unigrams_news)))\n",
        "\n",
        "id2word_news_1 = list(unigrams_news)\n",
        "word2id_news_1 = {word:i for i, word in enumerate(id2word_news_1)}\n",
        "id2word_news_2 = list(bigrams_news)\n",
        "word2id_news_2 = {word:i for i, word in enumerate(id2word_news_2)}\n",
        "\n",
        "for ngram in trigrams_news:\n",
        "    word1, word2, word3 = ngram.split()\n",
        "    bigram = ' '.join([word1, word2])\n",
        "    unigram = word3\n",
        "    matrix_news[word2id_news_2[bigram], word2id_news_1[unigram]] =  (trigrams_news[ngram]/bigrams_news[bigram])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "matrix_news"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmN4xwr1AnQh",
        "outputId": "aa524668-1df7-4bd4-fea2-331feddffd81"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<380345x71987 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 563892 stored elements in List of Lists format>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgHqBAP_QYea"
      },
      "source": [
        "Для генерации нам понадобится функция np.random.choice , которая выбирает случайный объект из заданных. Ещё в неё можно подать вероятность каждого объекта и она будет доставать по ним (не только максимальный по вероятности)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "TUnBRyhZQYea"
      },
      "outputs": [],
      "source": [
        "def generate(matrix, id2word, word2id, n=100, start='<start> <start>'):\n",
        "    text = []\n",
        "    current_idx = word2id[start]\n",
        "    current_bigram = start\n",
        "\n",
        "    for i in range(n):\n",
        "\n",
        "        chosen = np.random.choice(matrix.shape[1], p=matrix[current_idx].toarray()[0])\n",
        "        # print(current_bigram.split(), chosen, id2word[chosen])\n",
        "        text.append(id2word[chosen])\n",
        "        current_bigram = f'{current_bigram.split()[1]} {id2word[chosen]}'\n",
        "\n",
        "\n",
        "        if id2word[chosen] == '<end>':\n",
        "            chosen = word2id[start]\n",
        "            current_bigram = start\n",
        "        current_idx = word2id[current_bigram]\n",
        "\n",
        "    return ' '.join(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "366N3yvTQYeb",
        "outputId": "b74aae72-9752-4fff-da78-908eb4baf9a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "в департаменте правительственной информации соответствующее распоряжение было подписано еще 17 человек погибших за время фунционирования горячей линии сообщать об этих проблемах дает заявление министра иностранных дел панамы приветствовал джимми картера как главу американской делегации деликатно не обратив внимание на незнакомых людей выносящих мусор в контейнеры на припаркованные к домам подозрительные грузовики \n",
            " касаясь дела о его отставке \n",
            " тогда фсбэшники появились перед журналистами мадлен олбрайт отметила что политические проблемы \n",
            " позвонки лежатодин к одному сохранность их идеальная \n",
            " таким образом чтобы не допустить дальнейшее осуществление неправомерных валютных трансфертов \n",
            " как сообщил интерфаксу сам арсаханов возглавляемая им международная организация готова подключиться\n"
          ]
        }
      ],
      "source": [
        "print(generate(matrix_news, id2word_news_1, word2id_news_2).replace('<end>', '\\n'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate(matrix_news, id2word_news_1, word2id_news_2).replace('<end>', '\\n'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltHmN4l0DLiu",
        "outputId": "a2df4f03-cfa2-46a8-fe4e-ecfa45c50176"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12 октября состоялось одновременное открытие двух интернет-центров в калининграде и \n",
            " а после выборов специализированный ресурс полит ру проявляют повышенный интерес к которым уже истрачено около 22 миллиардов до 19,5 миллиардов кубометров газа \n",
            " это крупнейшее из проводимых когда-либо исследований такого рода преступления не находятся в госпиталях в тяжелом состоянии передает reuters \n",
            " от урагана пострадали также литва и чехия примут участие известныеэстрадные исполнители \n",
            " представитель этой организации составляет более 104 миллионов рублей в месяц заявил он \n",
            " я убил более ста мальчиков и растворил части их тел в кислоте \n",
            " думаю это была самая крупная авария на гонках формула-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate(matrix_news, id2word_news_1, word2id_news_2).replace('<end>', '\\n'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdgP-TKmDS9q",
        "outputId": "c4565204-b344-4031-aea3-4b2105d6659f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "это означает что американцы установили систему прослушивания телефонов в нашем стиле работы и на банковские военные и другие на дополнительную проверку направлено более 15 пассажиров следующих в аэропорт горячую еду в пятницу утверждение президента ингушетии руслана аушева среди этих людей немало глубоких стариков и женщин которые мешают искать бандитов и террористов в чечне \n",
            " в этом году в минувший четверг в ходе следствия было установлено заказчиком книги является находящаяся в распоряжении трибунала команда судебно-медицинских экспертов из forrester research к 2003 году и зарегистрировано в установленном порядке оформить въездные документы в здание вошли заместитель главы администрации президента \n",
            " в качестве легального\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate(matrix_news, id2word_news_1, word2id_news_2).replace('<end>', '\\n'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1SqUyOpDZUr",
        "outputId": "f83667f2-dcf0-45c2-df60-1080a17c190c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "в интервью корреспонденту wall street journal europe сообщил о своем отношении к россии связанных с делом bank of new york post джексон поясняя свою точку зрения что продолжение разработок в области высоких технологий получат преимущество \n",
            " мстислав ростропович выступит также отдельно с виолончелистами и с кем и кем иностранные миссии собираются выступать посредниками \n",
            " горело трехэтажное здание старой постройки \n",
            " предварительное расследование показало что при подписании союзного договора вместе и энергично подчеркнул глава государства запретил совмещать посты заместителя министра в ходе телефонного разговора с московской патриархией уже шестой год проводят специальные мероприятия по решению чеченского конфликта в чечне погиб командир\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8pQjVuwQYeb"
      },
      "source": [
        "### Перплексия"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9riSZMrQYec"
      },
      "source": [
        "Простыми словами - нам нужно расчитать вероятность текста (мы это уже научились делать выше) и возвести ее в степень (-1/N), где N это количество слов в тексте."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "X8vsqsrFQYec"
      },
      "outputs": [],
      "source": [
        "def perplexity(logp, N):\n",
        "    return np.exp((-1/N) * logp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "LLsFAbs6QYed"
      },
      "outputs": [],
      "source": [
        "def compute_joint_proba(text, word_counts):\n",
        "    prob = 0\n",
        "    tokens = normalize(text)\n",
        "    for word in tokens:\n",
        "        if word in word_counts:\n",
        "            prob += (np.log(word_counts[word]/word_counts.total()))\n",
        "        else:\n",
        "            prob += np.log(2e-5)\n",
        "\n",
        "    return prob, len(tokens)\n",
        "\n",
        "\n",
        "def compute_join_proba_markov_assumption_bi(text, word_counts, bigram_counts):\n",
        "    prob = 0\n",
        "    tokens = normalize(text)\n",
        "    for ngram in ngrammer(['<start>'] + tokens + ['<end>'], 2):\n",
        "        word1, word2 = ngram.split()\n",
        "        if word1 in word_counts and ngram in bigram_counts:\n",
        "            prob += np.log(bigram_counts[ngram]/word_counts[word1])\n",
        "        else:\n",
        "            prob += np.log(2e-5)\n",
        "\n",
        "    return prob, len(tokens)\n",
        "\n",
        "def compute_join_proba_markov_assumption_tri(text, bigram_counts, trigram_counts):\n",
        "    prob = 0\n",
        "    tokens = normalize(text)\n",
        "    for ngram in ngrammer(['<start>'] + tokens + ['<end>'], 3):\n",
        "        word1, word2, word3 = ngram.split()\n",
        "        bigram = ' '.join([word1, word2])\n",
        "        unigram = word3\n",
        "        if bigram in bigram_counts and ngram in trigram_counts:\n",
        "            prob += np.log(trigram_counts[ngram]/bigram_counts[bigram])\n",
        "        else:\n",
        "            prob += np.log(2e-5)\n",
        "\n",
        "    return prob, len(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Немного отложенных текстов"
      ],
      "metadata": {
        "id": "Fh9CT5V9dX0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_news2 = [['<start>'] + ['<start>'] + normalize(text) + ['<end>'] + ['<end>'] for text in sent_tokenize(news[-6000:])]\n",
        "len(sentences_news2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_t2_7fAcDUr",
        "outputId": "8d3823b0-ce1c-4889-ff6f-0492b2b0f0a6"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "2PSwSwvsQYek",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "074d6ef0-6d7b-49a7-bac4-978ee3d95eb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Униграммная модель 24556.427269220414\n"
          ]
        }
      ],
      "source": [
        "ps = []\n",
        "for sent in sentences_news2:\n",
        "    prob, N = compute_joint_proba(' '.join(sent), unigrams_news)\n",
        "    ps.append(perplexity(prob, N))\n",
        "print('Униграммная модель', np.mean(ps))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "BZNXHNcHQYel",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5950efd-356e-42bc-fae7-bf601a13d0ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Биграммная модель 14886.525176766476\n"
          ]
        }
      ],
      "source": [
        "ps = []\n",
        "for sent in sentences_news2:\n",
        "    prob, N = compute_join_proba_markov_assumption_bi(' '.join(sent), unigrams_news, bigrams_news)\n",
        "    ps.append(perplexity(prob, N))\n",
        "print('Биграммная модель', np.mean(ps))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "0nrZRINiQYel",
        "outputId": "4fdebb1a-6bec-4c7f-9207-4585a6ae036e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Триграммная модель 32112.552946369142\n"
          ]
        }
      ],
      "source": [
        "ps = []\n",
        "for sent in sentences_news2:\n",
        "    prob, N = compute_join_proba_markov_assumption_tri(' '.join(sent), bigrams_news, trigrams_news)\n",
        "    ps.append(perplexity(prob, N))\n",
        "print('Триграммная модель', np.mean(ps))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAyshCpsQYee"
      },
      "source": [
        "Ожидалось, что перплексия биграмной будет меньше униграммной, а триграммная будет ещё меньше. Но с триграммной что-то не так пошло. Но в семинарской тетрадке тоже на корпусе неожиданно получилось."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание № 2\n",
        "\n",
        "Прочитайте главу про языковое моделирование в книге Журафски и Мартина - https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
        "\n",
        "- Что можно делать с проблемой несловарных слов? В семинаре мы просто использовали какое-то маленькое значение вероятности, а какие есть другие способы?\n",
        "\n",
        "Можно в тесте давать только известные слова, но так делать нехорошо.\n",
        "Можно все неизвестные слова заменять на \\<UNK>. У нас заранне может быть фиксированный словарь и то, что в train в него не входит станет словом \\<UNK>. Или мы составляем словарь из train и то, что встречается меньше n раз, делаем словом \\<UNK>. В итоге у него тоже будет частота.\n",
        "\n",
        "- Что такое сглаживание (smoothing)?\n",
        "\n",
        "Способ решить проблему нулевой вероятности ситуации. Если слово попалось в неизвестном контексте (в котором в train не встречалось), чтобы не ставить вероятность =0, можно сказать, что вероятность маленькая (но не 0), отняв немного вероятности у частотных ситуаций."
      ],
      "metadata": {
        "id": "sHi68oGqQd2_"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}